{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Act LLM Baseline\n",
        "\n",
        "\n",
        "Single agent with a single module with Act Prompting with just LLMs (no vision aspect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import ast\n",
        "import base64\n",
        "import requests\n",
        "import json, os\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# set parent directory to address relative imports\n",
        "directory = Path(os.getcwd()).absolute()\n",
        "sys.path.append(str(directory.parent.parent.parent))\n",
        "\n",
        "# import environment\n",
        "from AI2Thor.env_new import AI2ThorEnv\n",
        "from AI2Thor.base_env import convert_dict_to_string\n",
        "from AI2Thor.object_actions import get_closest_feasible_action, get_closest_object_id\n",
        "from AI2Thor.baselines.utils import Logger, AutoConfig\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "# save a json file with your openai api key in your\n",
        "# home folder as {\"my_openai_api_key\": \"INSERT API HERE\"}\n",
        "with open(os.path.expanduser(\"~\") + \"/openai_key.json\") as json_file:\n",
        "    key = json.load(json_file)\n",
        "    api_key = key[\"my_openai_api_key\"]\n",
        "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize environment parameters\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.num_agents = 2\n",
        "        self.scene = \"FloorPlan1\"\n",
        "        self.scene_name = \"FloorPlan1\"\n",
        "        self.model = \"gpt-4\"\n",
        "        self.horizon = 30\n",
        "        self.use_langchain = False\n",
        "        self.use_strict_format = True\n",
        "        self.use_obs_summariser = False\n",
        "        self.use_act_summariser = False\n",
        "        self.use_action_failure = True\n",
        "        self.use_shared_subtask = True\n",
        "        self.use_separate_subtask = False\n",
        "        self.use_future_message = True\n",
        "        self.forceAction = False\n",
        "        self.use_memory = True\n",
        "        self.use_plan = True\n",
        "        self.use_separate_memory = False\n",
        "        self.use_shared_memory = True\n",
        "        self.temperature = 0.7\n",
        "\n",
        "config = Config()\n",
        "env = AI2ThorEnv(config)\n",
        "env.controller.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Act PROMPT\n",
        "agent_name = [\"Alice\", \"Bob\"]\n",
        "PROMPT = f\"\"\"You are an excellent planner who is tasked with helping 2 embodied robots named {agent_name[0]} and {agent_name[1]} carry out a task. Both robots have a partially observable view of the environment. Hence they have to explore around in the environment to do the task.\n",
        "They can perform the following actions: [\"navigate to object <object_id>\", \"rotate in <rotation> direction\", \"pick up object <object_id>\", \"put object on <receptacle_id>\", \"open object <object_id>\", \"close object <object_id>\", \"slice object <object_id>\", “toggle object <object_id> on”, “toggle object <object_id> off”, \"clean object <object_id>\", \"look up by angle <angle>\", \"look down by angle <angle>\", “move in <translation> direction\", \"stay idle\", \"Done\"]\n",
        "Here \"Done\" is used when all the robots have completed the main task. Only use it when you think all the subtasks are complete.\n",
        "\"stay idle\" is used when you want the robot to stay idle for one time step. This could be used to wait for the other robot to complete its subtask. Use it only when you think it is necessary.\n",
        "Here <rotation> can be one of [\"Right\", \"Left\"].\n",
        "Here <angle> is the angle in degrees and can only be one of [30, 60, 90, 120, 150, 180].\n",
        "Here <translation> can be one of [\"Ahead\", \"Back\", \"Left\", \"Right”].\n",
        "\n",
        "You need to suggest the action that each robot should take at the current time step.\n",
        "\n",
        "### Important Notes ###\n",
        "* The robots can hold only one object at a time.\n",
        "For example: If {agent_name} is holding an apple, it cannot pick up another object until it puts the apple down.\n",
        "* Even if the robot can see objects, it might not be able to interact with them if they are too far away. Hence you will need to make the robot navigate closer to the objects they want to interact with.\n",
        "For example: An action like \"pick up <object_id>\" is feasible only if robot can see the object and is close enough to it. So you will have to navigate closer to it before you can pick it up.\n",
        "In some scenarios, the agents might not see the objects that they want to interact with. In such cases, you will have to make the robot explore the environment to find the object.\n",
        "In such scenarios you can use actions to rotate in place or look up / down or navigate to explore the environment.\n",
        "* If you open an object, please ensure that you close it before you navigate to a different place.\n",
        "* Opening object like drawers, cabinets, fridge can block the path of the robot. So open objects only when you think it is necessary.\n",
        "\n",
        "### INPUT FORMAT ###\n",
        "* You will get a description of the task robots are supposed to do.\n",
        "* You will get an image of the environment at the current time step from {agent_name[0]}'s perspective and {agent_name[1]}'s perspective as the observation input. To help you with detecting objects in the image, you will also get a list objects each agent is able to see in the environment. Here the objects are named as \"<object_name>_<object_id>\". \n",
        "* You will get a trace of the steps taken by the robots and the actions they took at each time step and whether it was successful or not.\n",
        "\n",
        "### OUTPUT FORMAT ###\n",
        "In your output, do not have any extra text or content outside of the python dictionary as below. Do NOT put any text, spaces, or enter keys (i.e. \"/n\") outside of it.\n",
        "Your output should ONLY be in the form of a python dictionary, without any reasoning or extra text, as shown below:\n",
        "{{\"{agent_name[0]}\": \"action to be taken by {agent_name[0]}\", \"{agent_name[1]}\": \"action to be taken by {agent_name[1]}\"}}\n",
        "\n",
        "For example: If you think {agent_name[0]} should pick up an apple and {agent_name[1]} should navigate to the fridge, you will have to give the output as:\n",
        "{{\"{agent_name[0]}\": \"pick up apple\", \"{agent_name[1]}\": \"navigate to fridge\"}}\n",
        "\n",
        "* NOTE: DO NOT OUTPUT ANYTHING EXTRA OTHER THAN WHAT HAS BEEN SPECIFIED\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_action_llm_input(env):\n",
        "    \"\"\"\n",
        "    Returns the input to the subtask LLM\n",
        "    ### INPUT FORMAT ###\n",
        "    {{Task: description of the task the robots are supposed to do,\n",
        "    {agent_name[i]}'s observation: list of objects the {agent_name[0]} is observing}}\n",
        "    \"\"\"\n",
        "    # extract the agent_name's observations based on how many agents there are\n",
        "    llm_input_feats = []\n",
        "    for i in range(env.num_agents):\n",
        "        agent_name = env.agent_names[i]\n",
        "        llm_input_feats.extend([agent_name + \"'s observation\", ])\n",
        "    return dict((k, env.input_dict[k]) for k in llm_input_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_obs_list_str(string_list):\n",
        "    my_list = ast.literal_eval(string_list)\n",
        "     #my_list = ['Stove_1', 'Drawer_1', 'Cabinet_1', 'Cabinet_4']\n",
        "    # Join all elements in the list with commas\n",
        "    result = ', '.join(my_list)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_prompt(user_prompt:str, step_num:int):\n",
        "    \"\"\"module_name: str \n",
        "        choose from planner, verifier, action\n",
        "    \"\"\"\n",
        "    system_prompt = PROMPT\n",
        "    user_prompt += f\"\\nStep {step_num}:\\n\"\n",
        "    out_dict = get_action_llm_input(env)\n",
        "    for i in range(env.num_agents):\n",
        "        agent = env.agent_names[i]\n",
        "        obs = convert_obs_list_str(out_dict[f\"{agent}'s observation\"])\n",
        "        user_prompt += f\"{agent} observes {obs}\\n\"\n",
        "    \n",
        "    # user_prompt = convert_dict_to_string(get_action_llm_input(env))\n",
        "    return system_prompt, user_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_image(image_path:str):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "    \n",
        "def get_action_llm_input(env):\n",
        "    \"\"\"\n",
        "    Returns the input to the subtask LLM\n",
        "    ### INPUT FORMAT ###\n",
        "    {{Task: description of the task the robots are supposed to do,\n",
        "    {agent_name[i]}'s observation: list of objects the {agent_name[0]} is observing}}\n",
        "    \"\"\"\n",
        "    # extract the agent_name's observations based on how many agents there are\n",
        "    llm_input_feats = [\"Task\"]\n",
        "    for i in range(env.num_agents):\n",
        "        agent_name = env.agent_names[i]\n",
        "        llm_input_feats.extend([agent_name + \"'s observation\", ])\n",
        "    return dict((k, env.input_dict[k]) for k in llm_input_feats)\n",
        "\n",
        "def prepare_payload(user_prompt, step_num:int):\n",
        "    \"\"\"# payload consists of \n",
        "    * the image from each agent's perspective\n",
        "    * the system prompt (which is constant)\n",
        "    * the user prompt (which changes based on the state)\n",
        "    This is then sent to the openai api to get the response (action or plan or verification of the plan)\n",
        "    \"\"\"\n",
        "    system_prompt, user_prompt = prepare_prompt(user_prompt, step_num)\n",
        "    base64_image = []\n",
        "    image_path = env.get_frame(0)\n",
        "    base64_image.append(encode_image(image_path))\n",
        "    image_path = env.get_frame(1)\n",
        "    base64_image.append(encode_image(image_path))\n",
        "    payload = {\n",
        "        \"model\": \"gpt-4-vision-preview\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": system_prompt},\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": user_prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image[0]}\"},\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image[1]}\"},\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 1000,\n",
        "        \"temperature\": config.temperature,\n",
        "    }\n",
        "    return payload, user_prompt\n",
        "\n",
        "\n",
        "def get_action(response):\n",
        "    response_dict = response.json()\n",
        "    # convert the string to a dict\n",
        "    # json_acceptable_string = response_dict[\"choices\"][0][\"message\"][\"content\"].replace(\"'\", \"\\\"\").replace(\"\\n\", \"\").replace(\"json\", \"\").replace(\"`\", \"\")\n",
        "    try:\n",
        "        output = response_dict[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except:\n",
        "        pprint(f\"error: choices not in response_dict\\n {response_dict}\\n\")\n",
        "    dict_match = re.search(r'\\{.*\\}', output, re.DOTALL)\n",
        "\n",
        "    if dict_match:\n",
        "        # Extract the dictionary from the matched string\n",
        "        return json.loads(dict_match.group())\n",
        "    else:\n",
        "        # parsing error\n",
        "        pprint(f\"response_dict\\n {response_dict}\\n\")\n",
        "        pprint(f\"output\\n {output}\\n\")\n",
        "        pprint(f\"dict_match\\n {dict_match}\\n\")\n",
        "\n",
        "def get_gpt_response(user_prompt, step_num:int):\n",
        "    payload, user_prompt = prepare_payload(user_prompt, step_num)\n",
        "    response = requests.post(\n",
        "    \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload\n",
        ")\n",
        "    return response, user_prompt\n",
        "\n",
        "\n",
        "#  \"look up by angle <angle>\", \"look down by angle <angle>\"\n",
        "def action_checker(actions):\n",
        "    \"\"\"\n",
        "    Get closest valid action\n",
        "\n",
        "    The action output from the model is in natural language.\n",
        "    This function will find the env feasible action which has the closest embedding \n",
        "    to the natural language action output from the model.\n",
        "    Eg: \"pick up the apple\" -> \"PickupObject(Apple_1)\"\n",
        "    \"\"\"\n",
        "    checked_actions = []\n",
        "    for act in actions:\n",
        "        act = get_closest_feasible_action(act)\n",
        "        action_type = act.split(\"(\")[0]\n",
        "        if action_type in ['PickupObject', 'PutObject', 'OpenObject', 'CloseObject', 'SliceObject', 'NavigateTo', 'ToggleObjectOn', 'ToggleObjectOff', 'CleanObject']:\n",
        "            act = get_closest_object_id(act, env.object_dict)\n",
        "        checked_actions.append(act)\n",
        "    return checked_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_prompt_post_action(user_prompt, action, action_successes):\n",
        "    for i in range(env.num_agents):\n",
        "        agent = env.agent_names[i]\n",
        "        user_prompt += f\"{agent}'s action was {action[i]} and it was {'successful' if action_successes[i] else 'unsuccessful'}\\n\"\n",
        "    return user_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modify parent directory to address relative imports in autoconfig file\n",
        "\n",
        "# Run all tasks and floorplans in config.json\n",
        "\n",
        "auto=AutoConfig()\n",
        "amt_tasks=auto.get_amt_tasks()\n",
        "for task_index in range(amt_tasks):\n",
        "\n",
        "    auto.set_task(task_index)\n",
        "    amt_floorplans=auto.get_amt_floorplans(task_index)\n",
        "\n",
        "    for fp_index in range(amt_floorplans):\n",
        "\n",
        "        auto.set_floorplan(fp_index)\n",
        "        timeout=auto.get_task_timeout()\n",
        "\n",
        "        max_retries = 3\n",
        "        retries = 0\n",
        "\n",
        "        while retries < max_retries:\n",
        "            try: # sometimes rare errors happen, so give each task and floorplan a few tries\n",
        "\n",
        "                env = AI2ThorEnv(auto.config())\n",
        "                task = auto.task_string()\n",
        "                d = env.reset(task=task)\n",
        "\n",
        "                user_prompt = f\"Task: {task}\"\n",
        "\n",
        "                logger=Logger('ActTest', env) # changed from Act to ActTest so that Act baselines are not overwritten\n",
        "                for step_num in range(1, timeout+1):\n",
        "                    response, user_prompt = get_gpt_response(user_prompt, step_num)\n",
        "                    outdict = get_action(response)\n",
        "                    # get closest feasible action\n",
        "                    action_texts = [outdict[agent_name[0]], outdict[agent_name[1]]]\n",
        "                    action = action_checker(action_texts)\n",
        "                    # execute action in environment\n",
        "                    d, action_successes = env.step(action)\n",
        "                    # update user prompt with action taken and its success\n",
        "                    user_prompt = prepare_prompt_post_action(user_prompt, action_texts, action_successes)\n",
        "                    # append to dataframe\n",
        "                    coverage = env.checker.get_coverage()\n",
        "                    transport_rate = env.checker.get_transport_rate()\n",
        "                    finished = env.checker.check_success()\n",
        "                    # logging\n",
        "                    logger.log_step(step=step_num, preaction=action_texts, action=action, success=action_successes, coverage=coverage, transport_rate=transport_rate, finished=finished)\n",
        "                    print('_'*50)\n",
        "                    print(f\"Step {step_num}\")\n",
        "                    print(f\"Completed Subtasks: \")\n",
        "                    print(\"\\n\".join(env.checker.subtasks_completed))\n",
        "                    # if the model outputs \"Done\" for both agents, break\n",
        "                    if all(status == 'Done' for status in action):\n",
        "                        break\n",
        "\n",
        "                env.controller.stop()\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred: {e}. Retrying...\")\n",
        "                env.controller.stop()\n",
        "                retries += 1\n",
        "                if retries >= max_retries:\n",
        "                    print(\"Max retries reached. Moving to next floorplan or task.\")\n",
        "                    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pprint(logger.summarize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run only the given task and floorplan\n",
        "\n",
        "df = pd.DataFrame(columns=['Step', 'Action', 'Success', 'Coverage', 'Transport Rate'])\n",
        "\n",
        "def append_row(df, step, action, success, coverage, transport_rate):\n",
        "    row = pd.DataFrame([[step, action, success, coverage, transport_rate]], columns=['Step', 'Action', 'Success', 'Coverage', 'Transport Rate'])\n",
        "    df = pd.concat([df, row])\n",
        "    return df\n",
        "\n",
        "env = AI2ThorEnv(config)\n",
        "task = \"Put the bread, lettuce, and tomato in the fridge\"\n",
        "d = env.reset(task=task)\n",
        "user_prompt = f\"Task: {task}\"\n",
        "\n",
        "logger=Logger('ActTest', env)\n",
        "for step_num in range(1, config.horizon+1):\n",
        "    response, user_prompt = get_gpt_response(user_prompt, step_num)\n",
        "    outdict = get_action(response)\n",
        "    # get closest feasible action\n",
        "    action_texts = [outdict[agent_name[0]], outdict[agent_name[1]]]\n",
        "    action = action_checker(action_texts)\n",
        "    # execute action in environment\n",
        "    d, action_successes = env.step(action)\n",
        "    # update user prompt with action taken and its success\n",
        "    user_prompt = prepare_prompt_post_action(user_prompt, action_texts, action_successes)\n",
        "    # append to dataframe\n",
        "    coverage = env.checker.get_coverage()\n",
        "    transport_rate = env.checker.get_transport_rate()\n",
        "    finished = env.checker.check_success()\n",
        "    # logging\n",
        "    df = append_row(df, step_num, action, action_successes, coverage, transport_rate)\n",
        "    logger.log_step(step=step_num, preaction=action_texts, action=action, success=action_successes, coverage=coverage, transport_rate=transport_rate, finished=finished)\n",
        "    print('_'*50)\n",
        "    print(f\"Step {step_num}\")\n",
        "    print(f\"Completed Subtasks: \")\n",
        "    print(\"\\n\".join(env.checker.subtasks_completed))\n",
        "    # if the model outputs \"Done\" for both agents, break\n",
        "    if all(status == 'Done' for status in action):\n",
        "        break\n",
        "\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
