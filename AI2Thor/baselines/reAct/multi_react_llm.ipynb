{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Agent ReAct LLM Baseline\n",
    "\n",
    "\n",
    "Multi- agent with a single module with ReAct Prompting with just LLMs (no vision aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import base64\n",
    "import requests\n",
    "import json, os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "directory = Path(os.getcwd()).absolute()\n",
    "sys.path.append(str(directory.parent.parent.parent))\n",
    "\n",
    "from AI2Thor.env_new import AI2ThorEnv\n",
    "from AI2Thor.base_env import convert_dict_to_string\n",
    "from AI2Thor.object_actions import get_closest_feasible_action, get_closest_object_id\n",
    "from AI2Thor.baselines.utils import logging, auto_config\n",
    "from AI2Thor.summarisers.obs_summariser_2 import ObsSummaryLLM\n",
    "\n",
    "# save a json file with your openai api key in your\n",
    "# home folder as {\"my_openai_api_key\": \"INSERT API HERE\"}\n",
    "with open(os.path.expanduser(\"~\") + \"/openai_key.json\") as json_file:\n",
    "    key = json.load(json_file)\n",
    "    api_key = key[\"my_openai_api_key\"]\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "df = pd.DataFrame(columns=['Step', 'Action', 'Success', 'Coverage', 'Transport Rate'])\n",
    "log = pd.DataFrame(columns=['Think','Action'])\n",
    "\n",
    "def append_row(df, step, action, success, coverage, transport_rate):\n",
    "    row = pd.DataFrame([[step, action, success, coverage, transport_rate]], columns=['Step', 'Action', 'Success', 'Coverage', 'Transport Rate'])\n",
    "    df = pd.concat([df, row])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# import re\n",
    "# import requests\n",
    "# import json, os\n",
    "# import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from pathlib import Path\n",
    "# import sys\n",
    "\n",
    "# # set parent directory to address relative imports\n",
    "# directory = Path(os.getcwd()).absolute()\n",
    "# sys.path.append(str(directory)) # note: no \".parent\" addition is needed for python (.py) files\n",
    "\n",
    "# # import environment\n",
    "# from AI2Thor.env_new import AI2ThorEnv\n",
    "# from AI2Thor.base_env import convert_dict_to_string\n",
    "# from AI2Thor.object_actions import get_closest_feasible_action, get_closest_object_id\n",
    "# from baseline_utils.logging import Logger\n",
    "# from AI2Thor.summarisers.obs_summariser_2 import ObsSummaryLLM\n",
    "\n",
    "# # save a json file with your openai api key in your\n",
    "# # home folder as {\"my_openai_api_key\": \"INSERT API HERE\"}\n",
    "# with open(os.path.expanduser(\"~\") + \"/openai_key.json\") as json_file:\n",
    "#     key = json.load(json_file)\n",
    "#     api_key = key[\"my_openai_api_key\"]\n",
    "# headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "# df = pd.DataFrame(columns=['Step', 'Action', 'Success', 'Coverage', 'Transport Rate'])\n",
    "# log = pd.DataFrame(columns=['Think','Action'])\n",
    "\n",
    "# def append_row(df, step, action, success, coverage, transport_rate):\n",
    "#     row = pd.DataFrame([[step, action, success, coverage, transport_rate]], columns=['Step', 'Action', 'Success', 'Coverage', 'Transport Rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct PROMPT\n",
    "agent_name = [\"Alice\", \"Bob\"]\n",
    "ReAct_PROMPT = f\"\"\"You are an excellent planner who is tasked with helping 2 embodied robots named {agent_name[0]} and {agent_name[1]} carry out a task. Both robots have a partially observable view of the environment. Hence they have to explore around in the environment to do the task.\n",
    "They can perform the following actions: [\"navigate to object <object_id>\", \"rotate in <rotation> direction\", \"pick up object <object_id>\", \"put object on <receptacle_id>\", \"open object <object_id>\", \"close object <object_id>\", \"slice object <object_id>\", “toggle object <object_id> on”, “toggle object <object_id> off”, \"clean object <object_id>\", \"look up by angle <angle>\", \"look down by angle <angle>\", “move in <translation> direction\", \"stay idle\", \"Done\"]\n",
    "Here \"Done\" is used when all the robots have completed the main task. Only use it when you think all the subtasks are complete.\n",
    "\"stay idle\" is used when you want the robot to stay idle for one time step. This could be used to wait for the other robot to complete its subtask. Use it only when you think it is necessary.\n",
    "Here <rotation> can be one of [\"Right\", \"Left\"].\n",
    "Here <angle> is the angle in degrees and can only be one of [30, 60, 90, 120, 150, 180].\n",
    "Here <translation> can be one of [\"Ahead\", \"Back\", \"Left\", \"Right”].\n",
    "\n",
    "You need to suggest the action that each robot should take at the current time step.\n",
    "\n",
    "### Important Notes ###\n",
    "* The robots can hold only one object at a time.\n",
    "For example: If {agent_name} is holding an apple, it cannot pick up another object until it puts the apple down.\n",
    "* Even if the robot can see objects, it might not be able to interact with them if they are too far away. Hence you will need to make the robot navigate closer to the objects they want to interact with.\n",
    "For example: An action like \"pick up <object_id>\" is feasible only if robot can see the object and is close enough to it. So you will have to navigate closer to it before you can pick it up.\n",
    "In some scenarios, the agents might not see the objects that they want to interact with. In such cases, you will have to make the robot explore the environment to find the object.\n",
    "In such scenarios you can use actions to rotate in place or look up / down or navigate to explore the environment.\n",
    "* If you open an object, please ensure that you close it before you navigate to a different place.\n",
    "* Opening object like drawers, cabinets, fridge can block the path of the robot. So open objects only when you think it is necessary.\n",
    "\n",
    "### INPUT FORMAT ###\n",
    "* You will get a description of the task robots are supposed to do.\n",
    "* You will get an image of the environment at the current time step from {agent_name[0]}'s perspective and {agent_name[1]}'s perspective as the observation input. To help you with detecting objects in the image, you will also get a list objects each agent is able to see in the environment. Here the objects are named as \"<object_name>_<object_id>\". \n",
    "* You will get a trace of the steps taken by the robots and the actions they took at each time step and whether it was successful or not.\n",
    "\n",
    "### OUTPUT FORMAT ###\n",
    "You are supposed to think and suggest the action each robot is supposed to take at the current time step. Before suggesting an action you need to think, which\n",
    "the requires that you reason over the inputs and logically reflect on the task, observation and course of actions needed to complete the task \n",
    "\n",
    "Output Requirements: At each time step you must ONLY output a PYTHON DICTIONARY of the following two elements: \n",
    "*First element: Key = \"Think\" | Value:(Type: String): A logical reflection of the best action to be taken given the inputs: task at hand, observations, and trace.\n",
    "\n",
    "*Second element: Key = \"Action\" | Value:(Type: Python Dictionary):\n",
    "The value should be in the form of a python dictionary as shown below.\n",
    "{{\"{agent_name[0]}\": \"action to be taken by {agent_name[0]}\", \"{agent_name[1]}\": \"action to be taken by {agent_name[1]}\"}}\n",
    "For example: If you think {agent_name[0]} should pick up an apple and {agent_name[1]} should navigate to the fridge, you will have to give the output as:\n",
    "{{\"{agent_name[0]}\": \"pick up apple\", \"{agent_name[1]}\": \"navigate to fridge\"}}\n",
    "\n",
    "Here is an example output:\n",
    "{{\"Think\": \"To solve the task, I need to find and put the apple. The apple is likely to be on the countertop or table. Then find the fridge.\", \n",
    "\"Action\": {{\"{agent_name[0]}\": \"pick up apple\", \"{agent_name[1]}\": \"navigate to fridge\"}}\n",
    "}}\n",
    "* NOTE: DO NOT OUTPUT ANYTHING EXTRA OTHER THAN WHAT HAS BEEN SPECIFIED\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_agents = 2\n",
    "        self.scene = \"FloorPlan1\"\n",
    "        self.scene_name = \"FloorPlan1\"\n",
    "        self.model = \"gpt-4\"\n",
    "        self.horizon = 30           # change this to 30\n",
    "        self.use_langchain = False\n",
    "        self.use_strict_format = True\n",
    "        self.use_obs_summariser = False\n",
    "        self.use_act_summariser = False\n",
    "        self.use_action_failure = True\n",
    "        self.use_shared_subtask = True\n",
    "        self.use_separate_subtask = False\n",
    "        self.use_future_message = True\n",
    "        self.forceAction = False\n",
    "        self.use_memory = True\n",
    "        self.use_plan = True\n",
    "        self.use_separate_memory = False\n",
    "        self.use_shared_memory = True\n",
    "        self.temperature = 0.7\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set correct directory for config.json:\n",
    "directory = Path(os.getcwd()).absolute()\n",
    "config_dir = str(directory.parent.parent.parent)\n",
    "\n",
    "with open(config_dir + \"/config.json\", 'r') as f:\n",
    "    config_dict=json.load(f)\n",
    "config_arr = config_dict['tasks']\n",
    "\n",
    "num_tasks = len(config_arr)\n",
    "print(num_tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_index = 7\n",
    "task = config_arr[task_index]['task_description']\n",
    "print(task)\n",
    "print(len(config_arr[task_index]['task_floorplans']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_index = 0\n",
    "floorplan_name = config_arr[task_index]['task_floorplans'][fp_index]\n",
    "print(floorplan_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.scene = floorplan_name\n",
    "config.scene_name = floorplan_name\n",
    "env = AI2ThorEnv(config)\n",
    "d = env.reset(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"Task: {task}\"\n",
    "# logger=Logger('ReAct', env)\n",
    "summariserLLM = ObsSummaryLLM(user_prompt, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_llm_input(env):\n",
    "    \"\"\"\n",
    "    Returns the input to the subtask LLM\n",
    "    ### INPUT FORMAT ###\n",
    "    {{Task: description of the task the robots are supposed to do,\n",
    "    {agent_name[i]}'s observation: list of objects the {agent_name[0]} is observing}}\n",
    "    \"\"\"\n",
    "    # extract the agent_name's observations based on how many agents there are\n",
    "    llm_input_feats = []\n",
    "    for i in range(env.num_agents):\n",
    "        agent_name = env.agent_names[i]\n",
    "        llm_input_feats.extend([agent_name + \"'s observation\", ])\n",
    "    return dict((k, env.input_dict[k]) for k in llm_input_feats)\n",
    "convert_dict_to_string(get_action_llm_input(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_obs_list_str(string_list):\n",
    "    my_list = ast.literal_eval(string_list)\n",
    "     #my_list = ['Stove_1', 'Drawer_1', 'Cabinet_1', 'Cabinet_4']\n",
    "    # Join all elements in the list with commas\n",
    "    result = ', '.join(my_list)\n",
    "    return result\n",
    "\n",
    "out_dict = get_action_llm_input(env)\n",
    "convert_obs_list_str(out_dict[f\"{agent_name[0]}'s observation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(user_prompt:str, step_num:int):\n",
    "    \"\"\"module_name: str \n",
    "        choose from planner, verifier, action\n",
    "    \"\"\"\n",
    "    system_prompt = ReAct_PROMPT\n",
    "    user_prompt += f\"\\nStep {step_num}:\\n\"\n",
    "    out_dict = get_action_llm_input(env)\n",
    "    for i in range(env.num_agents):\n",
    "        agent = env.agent_names[i]\n",
    "        obs = convert_obs_list_str(out_dict[f\"{agent}'s observation\"])\n",
    "        filtered_obs = summariserLLM.get_gpt_response(obs)\n",
    "        user_prompt += f\"{agent} observes {filtered_obs}\\n\"\n",
    "    \n",
    "    # user_prompt = convert_dict_to_string(get_action_llm_input(env))\n",
    "    return system_prompt, user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path:str):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "def get_action_llm_input(env):\n",
    "    \"\"\"\n",
    "    Returns the input to the subtask LLM\n",
    "    ### INPUT FORMAT ###\n",
    "    {{Task: description of the task the robots are supposed to do,\n",
    "    {agent_name[i]}'s observation: list of objects the {agent_name[0]} is observing}}\n",
    "    \"\"\"\n",
    "    # extract the agent_name's observations based on how many agents there are\n",
    "    llm_input_feats = [\"Task\"]\n",
    "    for i in range(env.num_agents):\n",
    "        agent_name = env.agent_names[i]\n",
    "        llm_input_feats.extend([agent_name + \"'s observation\", ])\n",
    "    return dict((k, env.input_dict[k]) for k in llm_input_feats)\n",
    "\n",
    "def prepare_payload(user_prompt, step_num:int):\n",
    "    \"\"\"# payload consists of \n",
    "    * the image from each agent's perspective\n",
    "    * the system prompt (which is constant)\n",
    "    * the user prompt (which changes based on the state)\n",
    "    This is then sent to the openai api to get the response (action or plan or verification of the plan)\n",
    "    \"\"\"\n",
    "    system_prompt, user_prompt = prepare_prompt(user_prompt, step_num)\n",
    "    base64_image = []\n",
    "    image_path = env.get_frame(0)\n",
    "    base64_image.append(encode_image(image_path))\n",
    "    image_path = env.get_frame(1)\n",
    "    base64_image.append(encode_image(image_path))\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": system_prompt},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image[0]}\"},\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image[1]}\"},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": config.temperature,\n",
    "    }\n",
    "    return payload, user_prompt\n",
    "\n",
    "\n",
    "def get_action(response):\n",
    "    response_dict = response.json()\n",
    "    # convert the string to a dict\n",
    "    # json_acceptable_string = response_dict[\"choices\"][0][\"message\"][\"content\"].replace(\"'\", \"\\\"\").replace(\"\\n\", \"\").replace(\"json\", \"\").replace(\"`\", \"\")\n",
    "    output = response_dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "    dict_match = re.search(r'\\{.*\\}', output)\n",
    "\n",
    "    if dict_match:\n",
    "        # Extract the dictionary from the matched string\n",
    "        return json.loads(dict_match.group())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_gpt_response(user_prompt, step_num:int):\n",
    "    payload, user_prompt = prepare_payload(user_prompt, step_num)\n",
    "    response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload\n",
    ")\n",
    "    return response, user_prompt\n",
    "\n",
    "\n",
    "#  \"look up by angle <angle>\", \"look down by angle <angle>\"\n",
    "def action_checker(actions):\n",
    "    \"\"\"\n",
    "    Get closest valid action\n",
    "\n",
    "    The action output from the model is in natural language.\n",
    "    This function will find the env feasible action which has the closest embedding \n",
    "    to the natural language action output from the model.\n",
    "    Eg: \"pick up the apple\" -> \"PickupObject(Apple_1)\"\n",
    "    \"\"\"\n",
    "    checked_actions = []\n",
    "    for act in actions:\n",
    "        act = get_closest_feasible_action(act)\n",
    "        action_type = act.split(\"(\")[0]\n",
    "        if action_type in ['PickupObject', 'PutObject', 'OpenObject', 'CloseObject', 'SliceObject', 'NavigateTo', 'ToggleObjectOn', 'ToggleObjectOff', 'CleanObject']:\n",
    "            act = get_closest_object_id(act, env.object_dict)\n",
    "        checked_actions.append(act)\n",
    "    return checked_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_post_action(user_prompt, action_text, action, action_successes):\n",
    "    for i in range(env.num_agents):\n",
    "        agent = env.agent_names[i]\n",
    "        user_prompt += (\n",
    "            f\"{agent}'s intended action was {action_text[i]}, but it was {'successful' if action_successes[i] else 'unsuccessful'} \"\n",
    "            f\"in executing the {action}\\n\"\n",
    "        )\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df(log, think, action):\n",
    "    row = pd.DataFrame([[think, action]], columns=['Think','Action'])\n",
    "    newlog = pd.concat([log, row ])\n",
    "    return newlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_action(response):\n",
    "    global log\n",
    "    dict = response.json()\n",
    "    output = dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    match = re.search(r'({.*})', output, re.DOTALL)\n",
    "    if match:\n",
    "        json_string = match.group(0)\n",
    "        parse_dict = json.loads(json_string)\n",
    "        log = append_df(log, parse_dict[\"Think\"], parse_dict[\"Action\"])\n",
    "        # think_action.append(parse_dict)\n",
    "        return parse_dict[\"Action\"]\n",
    "    else:\n",
    "        # error.append(output)\n",
    "        raise Exception(\"No match found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step_num in range(1, config.horizon+1):\n",
    "    response, user_prompt = get_gpt_response(user_prompt, step_num)\n",
    "    outdict = parse_action(response)\n",
    "    # get closest feasible action\n",
    "    action_texts = [outdict[agent_name[0]], outdict[agent_name[1]]] # is what the LLM wants the agent to do\n",
    "    action = action_checker(action_texts)\n",
    "    # execute action in environment\n",
    "    d, action_successes = env.step(action)\n",
    "    # update user prompt with action taken and its success\n",
    "    user_prompt = prepare_prompt_post_action(user_prompt, action_texts, action, action_successes)  # include the actual action \n",
    "    # append to dataframe\n",
    "    coverage = env.checker.get_coverage()\n",
    "    transport_rate = env.checker.get_transport_rate()\n",
    "    finished = env.checker.check_success()\n",
    "    df = append_row(df, step_num, action, action_successes, coverage, transport_rate)\n",
    "    # logger.log_step(step=step_num, action=action, success=action_successes, coverage=coverage, transport_rate=transport_rate, finished=finished)\n",
    "    # if the model outputs \"Done\" for both agents, break\n",
    "    print('_'*50)\n",
    "    print(f\"Step {step_num}\")\n",
    "    print(f\"Completed Subtasks: \")\n",
    "    print(\"\\n\".join(env.checker.subtasks_completed))\n",
    "    if all(status == 'Done' for status in action):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "log.reset_index(drop=True, inplace=True)\n",
    "log.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_checker(['look up by angle 60', 'ToggleObjectOff(Faucet_1)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_checker(['look up by angle 90', 'ToggleObjectOff(Faucet_1)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest_feasible_action('ToggleObjectOff(Faucet_1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI2Thor.object_actions import all_actions\n",
    "\n",
    "pprint(all_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(env.action_dir_path / str(\"Multi_ReAct_FP1.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match = re.search(r'({.*})', output, re.DOTALL)\n",
    "# if match:\n",
    "#     json_string = match.group(0)\n",
    "#     parse_dict = json.loads(json_string)\n",
    "\n",
    "# print(parse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict =response.json()\n",
    "# output = dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "# print(output)\n",
    "# # print(type(output))\n",
    "\n",
    "\n",
    "# output_dict = json.loads(output)\n",
    "# # print(output_dict)\n",
    "# # dict_match = re.search(r\"\\{.*\\}\", output)\n",
    "# # out = json.loads(dict_match.group())\n",
    "# # print(dict)\n",
    "# # print(out)\n",
    "# # parse_action(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from baseline_utils.auto_config import AutoConfig\n",
    "\n",
    "# auto=AutoConfig()\n",
    "# amt_tasks=auto.get_amt_tasks()\n",
    "# for task_index in range(1):\n",
    "#     auto.set_task(task_index) # set task index\n",
    "#     amt_floorplans=auto.get_amt_floorplans(task_index)\n",
    "\n",
    "#     for fp_index in range(amt_floorplans):\n",
    "#         auto.set_floorplan(fp_index) # set floorplan\n",
    "#         timeout=auto.get_task_timeout() # get timeout number\n",
    "\n",
    "#         env = AI2ThorEnv(auto.config())\n",
    "#         task = auto.task_string()\n",
    "#         d = env.reset(task=task)\n",
    "#         # do things ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize environment parameters\n",
    "# class Config:\n",
    "#     def __init__(self):\n",
    "#         self.num_agents = 2\n",
    "#         self.scene = \"FloorPlan5\"\n",
    "#         self.scene_name = \"FloorPlan5\"\n",
    "#         self.model = \"gpt-4\"\n",
    "#         self.horizon = 30           # change this to 30\n",
    "#         self.use_langchain = False\n",
    "#         self.use_strict_format = True\n",
    "#         self.use_obs_summariser = False\n",
    "#         self.use_act_summariser = False\n",
    "#         self.use_action_failure = True\n",
    "#         self.use_shared_subtask = True\n",
    "#         self.use_separate_subtask = False\n",
    "#         self.use_future_message = True\n",
    "#         self.forceAction = False\n",
    "#         self.use_memory = True\n",
    "#         self.use_plan = True\n",
    "#         self.use_separate_memory = False\n",
    "#         self.use_shared_memory = True\n",
    "#         self.temperature = 0.7\n",
    "# config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task=\"put tomato, lettuce and bread in the fridge\"\n",
    "# env = AI2ThorEnv(config)\n",
    "# d = env.reset(task=task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
